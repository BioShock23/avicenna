{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce1a4c68-2925-4e3c-8b35-739ac327eab7",
   "metadata": {},
   "source": [
    "# Tests4Py Benchmark: MarkUp\n",
    "This notebook handles benchmark tests for MarkUp in the Tests4Py framework.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e39269d8-0cae-4628-83fd-8829da6002c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress logging for the notebook; uncomment the last line to disable Avicenna logs\n",
    "import logging\n",
    "\n",
    "# This will disable all logging messages\n",
    "logging.disable(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c321afbe-9552-472c-852a-27ea9e937efe",
   "metadata": {},
   "source": [
    "### Build Program from Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7498b12b-61ca-46a8-a056-7f76ab73a46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from tests4py.api.logging import deactivate\n",
    "deactivate()\n",
    "\n",
    "from debugging_benchmark.tests4py_benchmark.repository import MarkUpBenchmarkRepository\n",
    "\n",
    "# Initialize the benchmark repository and select the first program\n",
    "repository = MarkUpBenchmarkRepository()\n",
    "programs = repository.build()\n",
    "program = programs[0]  # Assuming there is only one MarkUp Subject; we use markup_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db46003c",
   "metadata": {},
   "source": [
    "### Initialize Avicenna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6db3fee",
   "metadata": {},
   "source": [
    "Initialize the `Avicenna` diagnostic system with specific parameters including minimum recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2a8ba40-ccab-4d8d-8e22-5922eb349d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Avicenna instance with configurations for diagnosis\n",
    "from avicenna import Avicenna\n",
    "\n",
    "# Convert program to dictionary format for Avicenna initialization\n",
    "param = program.to_dict()\n",
    "\n",
    "# Initialize Avicenna with a minimum recall configuration\n",
    "avicenna = Avicenna(\n",
    "    **param,\n",
    "    min_recall=0.7,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5503b3-bc26-4fe5-af1b-ad14b527e8bc",
   "metadata": {},
   "source": [
    "### Diagnosis Execution and Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d63f436a-9ae6-48d2-b130-32c871ea385e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagnosis complete.\n"
     ]
    }
   ],
   "source": [
    "# Perform the diagnosis using Avicenna and store the results\n",
    "from typing import Tuple\n",
    "from isla.language import Formula\n",
    "\n",
    "try:\n",
    "    diagnosis = avicenna.explain()\n",
    "    print(\"Diagnosis complete.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during diagnosis: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caca1a0b-958c-4d56-8c03-6035e884496c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avicenna determined the following constraints for Tests4PyBenchmarkProgram(markup_1):\n",
      "\n",
      "exists <chars> elem_xy in start:\n",
      "  inside(elem_xy, start)\n",
      "Avicenna calculated a precision of 26.19% and a recall of 100.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from isla.language import ISLaUnparser\n",
    "\n",
    "failure_diagnosis = diagnosis.pop(0)\n",
    "    \n",
    "print(f\"Avicenna determined the following constraints for {program}:\\n\")\n",
    "print(ISLaUnparser(failure_diagnosis.formula).unparse())\n",
    "print(f\"Avicenna calculated a precision of {failure_diagnosis.precision()*100:.2f}% and a recall of {failure_diagnosis.recall()*100:.2f}%\", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb1f554-1e0f-42ae-91fc-a634595719ce",
   "metadata": {},
   "source": [
    "### Super low Precision, Let's try a different Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63501f97-0b5a-452b-a0f7-43682aadd558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exists <char> elem in start:\n",
      "  (= elem \"\\\"\")\n",
      "Avicenna calculated a precision of 86.96% and a recall of 90.91%\n",
      "\n",
      "(exists <char> elem in start:\n",
      "   (= elem \"\\\"\") and\n",
      "forall <html> container in start:\n",
      "  exists <open> elem_0 in container:\n",
      "    (= (str.len elem_0) (str.to.int \"2\")))\n",
      "Avicenna calculated a precision of 93.33% and a recall of 63.64%\n",
      "\n",
      "(exists <char> elem in start:\n",
      "   (= elem \"\\\"\") and\n",
      "forall <html> container in start:\n",
      "  exists <open> elem_0 in container:\n",
      "    (= elem_0 \"<>\"))\n",
      "Avicenna calculated a precision of 93.33% and a recall of 63.64%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from avicenna.learning.metric import F1ScoreFitness\n",
    "\n",
    "candidates = avicenna.learner.get_candidates()\n",
    "strat = F1ScoreFitness()\n",
    "sorted_ = sorted(candidates, key=lambda c: strat.evaluate(c), reverse=True)\n",
    "\n",
    "for dia in sorted_[:3]:\n",
    "    print(ISLaUnparser(dia.formula).unparse())\n",
    "    print(f\"Avicenna calculated a precision of {dia.precision()*100:.2f}% and a recall of {dia.recall()*100:.2f}%\", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a384e7c-4d37-457f-a05e-eca400fd4d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "failure_diagnosis = sorted_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e8018c-3d93-4922-8981-31991a31502d",
   "metadata": {},
   "source": [
    "The constraint: \n",
    "\n",
    "```\n",
    "exists <char> elem in start:\n",
    "    (= elem \"\\\"\")\n",
    "```\n",
    "\n",
    "means that the error is predicted when there is a double quote character (`\"`) in the evaluated string or text elements. The diagnosis suggests that the presence of a double quote might be triggering errors, likely due to how these characters are handled or expected within the markup context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba8eb4f-fb2f-4d40-ab0a-e2ef69832511",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa88542-bc77-4e10-bc60-9c974586821b",
   "metadata": {},
   "source": [
    "### Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58e8dc7-f3fe-4c95-acfa-109250417b07",
   "metadata": {},
   "source": [
    "Generate test inputs using a grammar-based fuzzer, and classify these inputs as passing or failing based on the learned constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "617e75cf-4e0b-407b-821a-b734fd508031",
   "metadata": {},
   "outputs": [],
   "source": [
    "from debugging_framework.fuzzingbook.fuzzer import GrammarFuzzer\n",
    "from debugging_framework.input.input import Input, OracleResult\n",
    "\n",
    "def generate_inputs(grammar, num_inputs=1000):\n",
    "    fuzzer = GrammarFuzzer(grammar)\n",
    "    evaluation_data_set = set()\n",
    "\n",
    "    while len(evaluation_data_set) < num_inputs:\n",
    "        tree = fuzzer.fuzz()\n",
    "        evaluation_data_set.add(Input.from_str(grammar=grammar, input_string=tree))\n",
    "\n",
    "    return evaluation_data_set\n",
    "\n",
    "def classify_inputs(program, evaluation_data_set):\n",
    "    oracle = program.get_oracle()\n",
    "    failing, passing = set(), set()\n",
    "\n",
    "    for inp in evaluation_data_set:\n",
    "        oracle_result, exception = oracle(inp)\n",
    "        if oracle_result == OracleResult.FAILING:\n",
    "            failing.add(inp)\n",
    "        elif oracle_result == OracleResult.PASSING:\n",
    "            passing.add(inp)\n",
    "\n",
    "    return passing, failing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98dbe4c2-4659-4edc-b9d3-1d6718fedb91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1000 unique inputs for evaluation.\n",
      "Generated 963 passing inputs for evaluation!\n",
      "Generated 37 failing inputs for evaluation!\n"
     ]
    }
   ],
   "source": [
    "grammar = program.get_grammar()\n",
    "evaluation_data_set = generate_inputs(grammar)\n",
    "passing, failing = classify_inputs(program, evaluation_data_set)\n",
    "\n",
    "print(f\"Generated {len(evaluation_data_set)} unique inputs for evaluation.\")\n",
    "print(f\"Generated {len(passing)} passing inputs for evaluation!\")\n",
    "print(f\"Generated {len(failing)} failing inputs for evaluation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9c9a35-2075-410b-b33c-5ef4cf85de24",
   "metadata": {},
   "source": [
    "Calculate and display the precision and recall for the diagnostic results based on the test evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f593b943-20a9-4355-9b44-91f966673a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Diagnosis achieved a Precision of 90.00% a Recall of 48.65%\n"
     ]
    }
   ],
   "source": [
    "from isla.evaluator import evaluate\n",
    "\n",
    "# Calculate Precision and Recall\n",
    "tp = sum(bool(evaluate(failure_diagnosis.formula, inp.tree, grammar)) for inp in failing)\n",
    "fn = len(failing) - tp\n",
    "fp = sum(bool(evaluate(failure_diagnosis.formula, inp.tree, grammar)) for inp in passing)\n",
    "\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "print(f\"The Diagnosis achieved a Precision of {precision*100:.2f}% \" +\n",
    "      f\"a Recall of {recall*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cc2880-1544-4818-aa2b-401daca59709",
   "metadata": {},
   "source": [
    "### Producer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411f1efa-bf16-4eb9-8ec5-64a3a2acc145",
   "metadata": {},
   "source": [
    "#### Generating more Failing Inputs from Diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "109a5d8f-4825-4039-ade8-c78a58c9424b",
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = \"\"\"\n",
    "exists <char> elem in start:\n",
    "    (= elem \"\\\\\"\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5b24a5b-f14c-46eb-bc00-b43298201415",
   "metadata": {},
   "outputs": [],
   "source": [
    "from isla.solver import ISLaSolver\n",
    "\n",
    "solver = ISLaSolver(\n",
    "    grammar,\n",
    "    formula=formula,\n",
    "    enable_optimized_z3_queries=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3825653-e540-4ac0-a5d1-7b2d0ea306a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "failing_inputs = []\n",
    "for _ in range(100):\n",
    "    try:\n",
    "        inp = solver.solve()\n",
    "        failing_inputs.append(inp)\n",
    "        # print(str(inp).ljust(30), oracle(inp))\n",
    "    except StopIteration:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "694f6fa0-0bcd-4335-8f43-f4d87c9639cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "oracle = program.get_oracle()\n",
    "\n",
    "producer_failing: List[bool] = []\n",
    "for inp in failing_inputs:\n",
    "    oracle_result, exception = oracle(inp)\n",
    "    producer_failing.append(\n",
    "        oracle_result.is_failing()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d78dbc50-6801-49a7-8a37-87ec847db7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 100 inputs which are expected to be failing. (0 inputs are passing)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Generated {len(failing_inputs)} inputs which are expected to be failing. ({sum(not(inp) for inp in producer_failing)} inputs are passing)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f08367-2dc3-45bc-a625-347fe7f4f659",
   "metadata": {},
   "source": [
    "#### Generating Passing Inputs by Negating Constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dfc9a931-7764-4707-b6d5-2b4d0930d017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negated Constraint\n",
    "formula = \"\"\"\n",
    "not(exists <char> elem in start:\n",
    "    (= elem \"\\\\\"\"))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55ce38d7-7c35-4981-a215-0b02601ba21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from isla.solver import ISLaSolver\n",
    "\n",
    "solver = ISLaSolver(\n",
    "    grammar,\n",
    "    formula=formula,\n",
    "    enable_optimized_z3_queries=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e39a0be8-49bc-4d0c-a13b-b27fae2e6491",
   "metadata": {},
   "outputs": [],
   "source": [
    "passing_inputs = []\n",
    "for _ in range(100):\n",
    "    try:\n",
    "        inp = solver.solve()\n",
    "        passing_inputs.append(inp)\n",
    "        # print(str(inp).ljust(30), oracle(inp))\n",
    "    except StopIteration:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e6cf0171-4af9-4c34-96d4-870a35ac7902",
   "metadata": {},
   "outputs": [],
   "source": [
    "oracle = program.get_oracle()\n",
    "\n",
    "producer_passing: List[bool] = []\n",
    "for inp in passing_inputs:\n",
    "    oracle_result, exception = oracle(inp)\n",
    "    producer_passing.append(\n",
    "        oracle_result.is_failing()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ee21fa68-3a36-4a0b-938e-1abdf1c76e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 11 inputs which are expected to be passing. (1 inputs are failing)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Generated {len(passing_inputs)} inputs which are expected to be passing. ({sum(producer_passing)} inputs are failing)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e14f59a9-5acc-445e-afcb-5474cafe94f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Producer Evaluation:\n",
      "The Diagnosis achieved a Precision of 99.01% a Recall of 100.00%\n"
     ]
    }
   ],
   "source": [
    "from isla.evaluator import evaluate\n",
    "\n",
    "# Calculate Precision and Recall\n",
    "tp = sum(inp for inp in producer_failing)\n",
    "fn = len(producer_failing) - tp\n",
    "fp = sum(inp for inp in producer_passing)\n",
    "\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "print(f\"Producer Evaluation:\")\n",
    "print(f\"The Diagnosis achieved a Precision of {precision*100:.2f}% \" +\n",
    "      f\"a Recall of {recall*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a6b9cc-1231-4ffb-95a7-3bca1ff2feb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
